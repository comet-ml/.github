# Comet · AI Observability and Evaluation

Comet builds tools to help teams **evaluate**, **monitor**, and **understand** machine learning models and LLM applications across their lifecycle.

This organization hosts both our classic ML tools and our next-generation LLM observability frameworks — including:

- **Opik** – open-source framework for evaluating LLM applications
- **Comet ML** – experiment tracking, model monitoring, and artifacts
- **MCP** – our service supporting the Model Context Protocol

---

## Projects

### [Opik](https://github.com/comet-ml/opik)
> Open-source observability framework for LLM pipelines

Opik is a developer-first toolkit to trace, evaluate, and monitor large language model applications, RAG pipelines, and agents.

It supports:
- Function-level and chain-level tracing
- Built-in and custom evaluation metrics
- Integration with LangChain, LlamaIndex, or vanilla Python
- Export to Comet UI or your own dashboards

Ideal for debugging, fine-tuning, and QA of generative AI systems.

---

### [Comet ML](https://www.comet.com)
> Machine learning observability platform

The original Comet platform powers traditional ML workflows with:
- Experiment tracking
- Model registry
- Dataset & artifact versioning
- Production model monitoring (MPM)

It is used by thousands of data scientists to compare runs, track models, and manage experiments at scale.

---

## Get Involved

- Visit [comet.com](https://www.comet.com) for documentation and signup
- Explore [Opik](https://github.com/comet-ml/opik) and start evaluating LLMs
- Check out the [MCP standard](https://modelcontextprotocol.io) and integrate it into your agents
- Join the conversation on [Slack](https://www.comet.com/slack) or follow us on [X](https://twitter.com/cometml)

---

Comet is on a mission to bring clarity to AI development. Whether you're building traditional ML pipelines or complex LLM systems, we provide the tools to ship better models, faster.
